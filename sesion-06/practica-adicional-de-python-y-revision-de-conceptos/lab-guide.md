# Pr√°ctica adicional de Python y revisi√≥n de conceptos

## Informaci√≥n general del laboratorio

Este laboratorio intensivo est√° dise√±ado para fortalecer sus fundamentos en Python y Machine Learning a trav√©s de ejercicios pr√°cticos progresivos. Cubriremos desde conceptos b√°sicos de programaci√≥n hasta implementaciones avanzadas de algoritmos de ML, con un enfoque especial en preparaci√≥n para proyectos de Machine Learning en producci√≥n.

La sesi√≥n combina teor√≠a y pr√°ctica, con ejercicios interactivos que van desde manipulaci√≥n de datos hasta implementaci√≥n de modelos completos, todo en el contexto de casos de uso reales de la industria.

## Objetivos

Al finalizar este laboratorio, podr√° realizar lo siguiente:

- Dominar estructuras de datos y control de flujo en Python
- Manipular datos eficientemente con NumPy y Pandas
- Implementar algoritmos de Machine Learning desde cero
- Optimizar c√≥digo Python para rendimiento
- Manejar grandes vol√∫menes de datos
- Implementar pipelines de ML completos
- Debuggear y probar c√≥digo de ML efectivamente
- Aplicar mejores pr√°cticas de programaci√≥n en Python

## Duraci√≥n

El tiempo estimado para completar este laboratorio es de **80 minutos**.

## M√≥dulo 1: Fundamentos de Python (20 minutos)

### Ejercicio 1.1: Estructuras de datos avanzadas

```python
"""
Ejercicio: Sistema de gesti√≥n de inventario
Implemente un sistema que maneje el inventario de una tienda usando
diferentes estructuras de datos de Python.
"""

class InventoryManager:
    def __init__(self):
        # TODO: Inicializar estructuras de datos apropiadas
        self.products = {}  # dict para productos
        self.categories = set()  # set para categor√≠as √∫nicas
        self.transactions = []  # list para historial
        self.suppliers = {}  # dict anidado para proveedores
    
    def add_product(self, product_id, name, category, price, quantity, supplier):
        """
        Agregar producto al inventario
        """
        # TODO: Implementar validaciones
        # TODO: Actualizar todas las estructuras de datos
        # TODO: Manejar casos edge (producto existente, etc.)
        pass
    
    def update_stock(self, product_id, quantity_change, transaction_type):
        """
        Actualizar stock y registrar transacci√≥n
        """
        # TODO: Validar producto existe
        # TODO: Actualizar cantidad
        # TODO: Registrar transacci√≥n con timestamp
        pass
    
    def get_low_stock_products(self, threshold=10):
        """
        Obtener productos con stock bajo
        """
        # TODO: Usar list comprehension
        # TODO: Filtrar por umbral
        # TODO: Ordenar por cantidad ascendente
        pass
    
    def generate_report(self):
        """
        Generar reporte completo del inventario
        """
        # TODO: Usar f-strings para formateo
        # TODO: Calcular estad√≠sticas agregadas
        # TODO: Mostrar top productos por categor√≠a
        pass

# Casos de prueba
def test_inventory_system():
    """
    Funci√≥n para probar el sistema de inventario
    """
    inventory = InventoryManager()
    
    # TODO: Agregar productos de prueba
    # TODO: Simular transacciones
    # TODO: Generar reportes
    # TODO: Validar resultados
    
    pass

# Ejecutar pruebas
test_inventory_system()
```

### Ejercicio 1.2: Programaci√≥n funcional y comprehensions

```python
"""
Ejercicio: An√°lisis de datos de ventas usando programaci√≥n funcional
"""

from functools import reduce
from itertools import groupby
import operator

# Datos de ejemplo
sales_data = [
    {'date': '2024-01-01', 'product': 'Laptop', 'category': 'Electronics', 
     'quantity': 2, 'price': 1200, 'salesperson': 'Alice'},
    {'date': '2024-01-01', 'product': 'Mouse', 'category': 'Electronics', 
     'quantity': 5, 'price': 25, 'salesperson': 'Bob'},
    {'date': '2024-01-02', 'product': 'Keyboard', 'category': 'Electronics', 
     'quantity': 3, 'price': 75, 'salesperson': 'Alice'},
    # ... m√°s datos
]

def functional_analysis(data):
    """
    Realizar an√°lisis usando programaci√≥n funcional
    """
    
    # TODO: 1. Calcular ingresos totales usando map y reduce
    total_revenue = reduce(
        operator.add,
        map(lambda x: x['quantity'] * x['price'], data)
    )
    
    # TODO: 2. Filtrar ventas de electr√≥nicos > $100
    high_value_electronics = list(filter(
        lambda x: x['category'] == 'Electronics' and x['quantity'] * x['price'] > 100,
        data
    ))
    
    # TODO: 3. Crear diccionario de ventas por vendedor usando comprehension
    sales_by_person = {
        person: sum(sale['quantity'] * sale['price'] 
                   for sale in data if sale['salesperson'] == person)
        for person in set(sale['salesperson'] for sale in data)
    }
    
    # TODO: 4. Agrupar por categor√≠a y calcular estad√≠sticas
    data_sorted = sorted(data, key=operator.itemgetter('category'))
    category_stats = {
        category: {
            'total_sales': sum(item['quantity'] * item['price'] for item in items),
            'avg_price': sum(item['price'] for item in items) / len(list(items)),
            'total_quantity': sum(item['quantity'] for item in items)
        }
        for category, items in groupby(data_sorted, key=operator.itemgetter('category'))
    }
    
    return {
        'total_revenue': total_revenue,
        'high_value_electronics': high_value_electronics,
        'sales_by_person': sales_by_person,
        'category_stats': category_stats
    }

# TODO: Implementar generadores para procesar datos grandes
def sales_data_generator(filename):
    """
    Generador para procesar archivos grandes l√≠nea por l√≠nea
    """
    # TODO: Implementar generador que lea CSV
    # TODO: Yield datos procesados uno por uno
    # TODO: Manejar errores de formato
    pass

# Ejercicio con decoradores
def timing_decorator(func):
    """
    Decorador para medir tiempo de ejecuci√≥n
    """
    import time
    from functools import wraps
    
    @wraps(func)
    def wrapper(*args, **kwargs):
        # TODO: Implementar medici√≥n de tiempo
        # TODO: Log del tiempo de ejecuci√≥n
        # TODO: Retornar resultado original
        pass
    return wrapper

@timing_decorator
def process_large_dataset(data):
    """
    Funci√≥n para procesar dataset grande
    """
    # TODO: Implementar procesamiento intensivo
    pass
```

## M√≥dulo 2: NumPy y manipulaci√≥n de arrays (15 minutos)

### Ejercicio 2.1: Operaciones vectorizadas avanzadas

```python
import numpy as np
import matplotlib.pyplot as plt

def numpy_advanced_operations():
    """
    Ejercicios avanzados con NumPy para ML
    """
    
    # TODO: 1. Crear dataset sint√©tico para regresi√≥n
    np.random.seed(42)
    n_samples = 1000
    n_features = 5
    
    # Generar caracter√≠sticas correlacionadas
    X = np.random.randn(n_samples, n_features)
    # TODO: Agregar correlaciones entre caracter√≠sticas
    # TODO: Crear target con ruido
    
    # TODO: 2. Implementar normalizaci√≥n manual
    def normalize_features(X):
        """
        Normalizar caracter√≠sticas usando broadcasting
        """
        # TODO: Calcular media y std por columna
        # TODO: Aplicar normalizaci√≥n Z-score
        # TODO: Manejar divisi√≥n por cero
        pass
    
    # TODO: 3. Implementar PCA desde cero
    def pca_from_scratch(X, n_components=2):
        """
        Implementar PCA usando operaciones de NumPy
        """
        # TODO: Centrar los datos
        # TODO: Calcular matriz de covarianza
        # TODO: Obtener eigenvalores y eigenvectores
        # TODO: Seleccionar componentes principales
        # TODO: Transformar datos
        pass
    
    # TODO: 4. Operaciones de √°lgebra lineal para ML
    def linear_regression_numpy(X, y):
        """
        Implementar regresi√≥n lineal usando NumPy
        """
        # TODO: Agregar columna de bias
        # TODO: Calcular coeficientes usando ecuaci√≥n normal
        # TODO: Implementar predicci√≥n
        # TODO: Calcular m√©tricas de error
        pass
    
    # TODO: 5. Operaciones con broadcasting para eficiencia
    def efficient_distance_matrix(X):
        """
        Calcular matriz de distancias eficientemente
        """
        # TODO: Usar broadcasting para evitar loops
        # TODO: Implementar distancia euclidiana
        # TODO: Comparar rendimiento con loops tradicionales
        pass
    
    return X, y

# Ejercicio de optimizaci√≥n
def numpy_optimization_techniques():
    """
    T√©cnicas de optimizaci√≥n con NumPy
    """
    
    # TODO: Comparar rendimiento de diferentes enfoques
    def compare_implementations():
        """
        Comparar loops vs vectorizaci√≥n vs broadcasting
        """
        # TODO: Implementar misma operaci√≥n de 3 formas
        # TODO: Medir tiempo de cada implementaci√≥n
        # TODO: Mostrar diferencias de rendimiento
        pass
    
    # TODO: Memory views y copy vs view
    def memory_efficiency_demo():
        """
        Demostrar manejo eficiente de memoria
        """
        # TODO: Mostrar diferencia entre copy y view
        # TODO: Usar memory_layout para optimizar
        # TODO: Implementar operaciones in-place
        pass
```

## M√≥dulo 3: Pandas avanzado para Data Science (20 minutos)

### Ejercicio 3.1: Manipulaci√≥n avanzada de DataFrames

```python
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

def advanced_pandas_operations():
    """
    Operaciones avanzadas de Pandas para Data Science
    """
    
    # TODO: 1. Crear dataset complejo multi-√≠ndice
    def create_complex_dataset():
        """
        Crear dataset con m√∫ltiples niveles y tipos de datos
        """
        np.random.seed(42)
        
        # TODO: Crear √≠ndice jer√°rquico (fecha, regi√≥n, producto)
        # TODO: Incluir diferentes tipos de datos
        # TODO: Agregar valores faltantes intencionalmente
        # TODO: Crear relaciones entre columnas
        pass
    
    # TODO: 2. Operaciones de groupby avanzadas
    def advanced_groupby_operations(df):
        """
        Operaciones complejas de agrupaci√≥n
        """
        # TODO: Multiple aggregations con nombres personalizados
        # TODO: Transform para crear caracter√≠sticas derivadas
        # TODO: Apply con funciones personalizadas complejas
        # TODO: Rolling windows y expanding windows
        pass
    
    # TODO: 3. Manejo avanzado de fechas y tiempo
    def time_series_operations(df):
        """
        Operaciones avanzadas con series temporales
        """
        # TODO: Resampling con funciones personalizadas
        # TODO: Shift y lag para crear caracter√≠sticas temporales
        # TODO: Detecci√≥n de estacionalidad
        # TODO: Interpolaci√≥n de valores faltantes
        pass
    
    # TODO: 4. Merge y join complejos
    def complex_data_joining():
        """
        Operaciones avanzadas de combinaci√≥n de datos
        """
        # TODO: Crear m√∫ltiples DataFrames relacionados
        # TODO: Merge con m√∫ltiples keys y condiciones
        # TODO: Concat con diferentes niveles
        # TODO: Manejar duplicados y conflictos
        pass
    
    # TODO: 5. Optimizaci√≥n de rendimiento
    def pandas_optimization_techniques(df):
        """
        T√©cnicas de optimizaci√≥n para Pandas
        """
        # TODO: Usar categor√≠as para strings repetitivos
        # TODO: Optimizar tipos de datos (downcast)
        # TODO: Chunking para datasets grandes
        # TODO: Usar query() para filtros complejos
        # TODO: Vectorizaci√≥n vs apply vs iterrows
        pass

# Ejercicio pr√°ctico: Pipeline de limpieza de datos
class DataCleaningPipeline:
    """
    Pipeline completo de limpieza de datos
    """
    
    def __init__(self):
        self.cleaning_steps = []
        self.metadata = {}
    
    def add_step(self, step_name, step_function):
        """
        Agregar paso de limpieza al pipeline
        """
        # TODO: Implementar sistema de pasos
        pass
    
    def detect_data_quality_issues(self, df):
        """
        Detectar autom√°ticamente problemas de calidad
        """
        # TODO: Detectar valores faltantes
        # TODO: Identificar outliers
        # TODO: Encontrar duplicados
        # TODO: Validar tipos de datos
        # TODO: Detectar inconsistencias
        pass
    
    def auto_clean(self, df):
        """
        Limpieza autom√°tica basada en heur√≠sticas
        """
        # TODO: Aplicar reglas de limpieza autom√°ticas
        # TODO: Registrar cambios realizados
        # TODO: Generar reporte de limpieza
        pass
    
    def validate_cleaned_data(self, df_original, df_cleaned):
        """
        Validar que la limpieza fue exitosa
        """
        # TODO: Comparar estad√≠sticas antes/despu√©s
        # TODO: Verificar integridad de datos
        # TODO: Generar m√©tricas de calidad
        pass
```

## M√≥dulo 4: Machine Learning desde cero (25 minutos)

### Ejercicio 4.1: Implementaci√≥n de algoritmos fundamentales

```python
import numpy as np
from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split

class LinearRegressionFromScratch:
    """
    Implementaci√≥n de regresi√≥n lineal desde cero
    """
    
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None
        self.cost_history = []
    
    def fit(self, X, y):
        """
        Entrenar el modelo usando gradient descent
        """
        # TODO: Inicializar par√°metros
        # TODO: Implementar gradient descent
        # TODO: Calcular y guardar cost en cada iteraci√≥n
        # TODO: Implementar early stopping opcional
        pass
    
    def predict(self, X):
        """
        Hacer predicciones
        """
        # TODO: Implementar predicci√≥n
        pass
    
    def compute_cost(self, y_true, y_pred):
        """
        Calcular funci√≥n de costo (MSE)
        """
        # TODO: Implementar MSE
        pass

class LogisticRegressionFromScratch:
    """
    Implementaci√≥n de regresi√≥n log√≠stica desde cero
    """
    
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None
        self.cost_history = []
    
    def sigmoid(self, z):
        """
        Funci√≥n sigmoid
        """
        # TODO: Implementar sigmoid con manejo de overflow
        pass
    
    def fit(self, X, y):
        """
        Entrenar modelo con gradient descent
        """
        # TODO: Implementar entrenamiento
        # TODO: Usar log-likelihood como cost function
        pass
    
    def predict(self, X, threshold=0.5):
        """
        Hacer predicciones binarias
        """
        # TODO: Implementar predicci√≥n
        pass
    
    def predict_proba(self, X):
        """
        Obtener probabilidades
        """
        # TODO: Implementar probabilidades
        pass

class KMeansFromScratch:
    """
    Implementaci√≥n de K-Means desde cero
    """
    
    def __init__(self, k=3, max_iters=100, random_state=42):
        self.k = k
        self.max_iters = max_iters
        self.random_state = random_state
    
    def initialize_centroids(self, X):
        """
        Inicializar centroides aleatoriamente
        """
        # TODO: Implementar inicializaci√≥n
        # TODO: Usar random_state para reproducibilidad
        pass
    
    def assign_clusters(self, X, centroids):
        """
        Asignar puntos al centroide m√°s cercano
        """
        # TODO: Calcular distancias
        # TODO: Asignar clusters
        pass
    
    def update_centroids(self, X, clusters):
        """
        Actualizar posici√≥n de centroides
        """
        # TODO: Calcular nuevos centroides
        # TODO: Manejar clusters vac√≠os
        pass
    
    def fit(self, X):
        """
        Ejecutar algoritmo K-Means
        """
        # TODO: Implementar loop principal
        # TODO: Criterio de convergencia
        # TODO: Guardar historial de centroides
        pass

# Ejercicio: Implementar validaci√≥n cruzada
class CrossValidatorFromScratch:
    """
    Implementaci√≥n de validaci√≥n cruzada desde cero
    """
    
    def __init__(self, cv=5, shuffle=True, random_state=42):
        self.cv = cv
        self.shuffle = shuffle
        self.random_state = random_state
    
    def split(self, X, y=None):
        """
        Generar splits para validaci√≥n cruzada
        """
        # TODO: Implementar K-fold splits
        # TODO: Manejar shuffle
        # TODO: Yield train/test indices
        pass
    
    def cross_val_score(self, estimator, X, y, scoring='accuracy'):
        """
        Calcular scores de validaci√≥n cruzada
        """
        # TODO: Implementar CV completa
        # TODO: Soportar diferentes m√©tricas
        # TODO: Retornar array de scores
        pass

# Ejercicio pr√°ctico: Comparar implementaciones
def compare_implementations():
    """
    Comparar implementaciones propias vs sklearn
    """
    # TODO: Generar datasets de prueba
    # TODO: Entrenar modelos propios y sklearn
    # TODO: Comparar rendimiento y precisi√≥n
    # TODO: Medir tiempos de ejecuci√≥n
    # TODO: Visualizar resultados
    pass
```

### Ejercicio 4.2: Pipeline de ML completo

```python
class MLPipelineFromScratch:
    """
    Pipeline completo de Machine Learning
    """
    
    def __init__(self):
        self.steps = []
        self.fitted_steps = []
        self.metadata = {}
    
    def add_step(self, name, transformer):
        """
        Agregar paso al pipeline
        """
        # TODO: Implementar sistema de pasos
        pass
    
    def fit(self, X, y=None):
        """
        Ajustar todos los pasos del pipeline
        """
        # TODO: Fit cada paso secuencialmente
        # TODO: Transform datos entre pasos
        # TODO: Guardar estado de cada paso
        pass
    
    def transform(self, X):
        """
        Aplicar transformaciones
        """
        # TODO: Aplicar transformaciones en orden
        pass
    
    def fit_transform(self, X, y=None):
        """
        Fit y transform en un paso
        """
        # TODO: Implementar fit_transform
        pass

# Transformadores personalizados
class OutlierRemover:
    """
    Remover outliers usando IQR
    """
    
    def __init__(self, factor=1.5):
        self.factor = factor
        self.lower_bounds = None
        self.upper_bounds = None
    
    def fit(self, X, y=None):
        # TODO: Calcular bounds usando IQR
        pass
    
    def transform(self, X):
        # TODO: Remover outliers
        pass

class FeatureEngineer:
    """
    Crear caracter√≠sticas derivadas
    """
    
    def __init__(self, polynomial_degree=2, interaction_terms=True):
        self.polynomial_degree = polynomial_degree
        self.interaction_terms = interaction_terms
    
    def fit(self, X, y=None):
        # TODO: Preparar transformaciones
        pass
    
    def transform(self, X):
        # TODO: Crear caracter√≠sticas polinomiales
        # TODO: Crear t√©rminos de interacci√≥n
        # TODO: Crear caracter√≠sticas estad√≠sticas
        pass
```

## M√≥dulo 5: Optimizaci√≥n y mejores pr√°cticas (20 minutos)

### Ejercicio 5.1: Optimizaci√≥n de c√≥digo Python

```python
import cProfile
import timeit
from memory_profiler import profile
import joblib
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

class PythonOptimizationTechniques:
    """
    T√©cnicas avanzadas de optimizaci√≥n en Python
    """
    
    @staticmethod
    def benchmark_function(func, *args, **kwargs):
        """
        Benchmark funci√≥n con m√∫ltiples m√©tricas
        """
        # TODO: Medir tiempo de ejecuci√≥n
        # TODO: Medir uso de memoria
        # TODO: Profile con cProfile
        # TODO: Retornar m√©tricas completas
        pass
    
    @staticmethod
    def optimize_loops():
        """
        Comparar diferentes enfoques para loops
        """
        # TODO: Loop tradicional vs comprehension vs map
        # TODO: Numpy vectorizado vs Python puro
        # TODO: Numba JIT compilation
        pass
    
    @staticmethod
    def memory_optimization_demo():
        """
        T√©cnicas de optimizaci√≥n de memoria
        """
        # TODO: Generators vs lists
        # TODO: __slots__ en clases
        # TODO: Memory mapping para archivos grandes
        # TODO: Garbage collection manual
        pass
    
    @staticmethod
    def parallel_processing_demo():
        """
        Procesamiento paralelo para ML
        """
        # TODO: Threading vs Multiprocessing
        # TODO: Joblib para ML paralelo
        # TODO: Async/await para I/O
        pass

# Ejercicio: Optimizar pipeline de ML
def optimize_ml_pipeline():
    """
    Optimizar pipeline completo de ML
    """
    
    # TODO: 1. Optimizar carga de datos
    def optimized_data_loading(filename):
        """
        Carga optimizada de datasets grandes
        """
        # TODO: Usar chunks para archivos grandes
        # TODO: Optimizar tipos de datos
        # TODO: Parallel reading
        # TODO: Caching inteligente
        pass
    
    # TODO: 2. Optimizar preprocesamiento
    def optimized_preprocessing(df):
        """
        Preprocesamiento optimizado
        """
        # TODO: Vectorizaci√≥n de operaciones
        # TODO: Memory-efficient transformations
        # TODO: Pipeline caching
        pass
    
    # TODO: 3. Optimizar entrenamiento
    def optimized_training(X, y):
        """
        Entrenamiento optimizado
        """
        # TODO: Early stopping
        # TODO: Warm start para modelos iterativos
        # TODO: Parallel model training
        # TODO: Hyperparameter optimization
        pass

# Testing y debugging
class MLTestingSuite:
    """
    Suite de testing para modelos de ML
    """
    
    @staticmethod
    def test_data_quality(df):
        """
        Tests autom√°ticos de calidad de datos
        """
        # TODO: Assert no null values donde no deber√≠an estar
        # TODO: Verificar rangos de valores
        # TODO: Comprobar distribuciones esperadas
        # TODO: Detectar data drift
        pass
    
    @staticmethod
    def test_model_performance(model, X_test, y_test):
        """
        Tests de rendimiento del modelo
        """
        # TODO: Verificar m√©tricas m√≠nimas
        # TODO: Test de overfitting
        # TODO: Consistency tests
        # TODO: Fairness tests
        pass
    
    @staticmethod
    def test_model_robustness(model, X_test):
        """
        Tests de robustez del modelo
        """
        # TODO: Adversarial examples
        # TODO: Input perturbation tests
        # TODO: Edge cases handling
        pass
```

## Ejercicios de integraci√≥n final

### Proyecto integrador: Sistema de recomendaciones

```python
class RecommendationSystem:
    """
    Sistema de recomendaciones completo usando conceptos aprendidos
    """
    
    def __init__(self):
        self.user_item_matrix = None
        self.item_features = None
        self.user_features = None
        self.models = {}
    
    def load_and_preprocess_data(self, ratings_file, items_file, users_file):
        """
        Cargar y preprocesar datos usando t√©cnicas optimizadas
        """
        # TODO: Implementar carga eficiente
        # TODO: Aplicar pipeline de limpieza
        # TODO: Feature engineering
        # TODO: Crear matrices sparse
        pass
    
    def train_collaborative_filtering(self):
        """
        Entrenar modelo colaborativo desde cero
        """
        # TODO: Implementar matrix factorization
        # TODO: Usar gradient descent optimizado
        # TODO: Regularizaci√≥n
        # TODO: Validaci√≥n cruzada
        pass
    
    def train_content_based(self):
        """
        Entrenar modelo basado en contenido
        """
        # TODO: TF-IDF para caracter√≠sticas de items
        # TODO: Similarity computation
        # TODO: Weighted recommendations
        pass
    
    def hybrid_recommendations(self, user_id, n_recommendations=10):
        """
        Combinar enfoques colaborativo y de contenido
        """
        # TODO: Obtener recomendaciones de ambos modelos
        # TODO: Weighted combination
        # TODO: Diversity injection
        # TODO: Novelty considerations
        pass
    
    def evaluate_system(self, test_data):
        """
        Evaluaci√≥n completa del sistema
        """
        # TODO: M√∫ltiples m√©tricas (RMSE, Precision@K, Recall@K)
        # TODO: Coverage analysis
        # TODO: Diversity metrics
        # TODO: A/B testing framework
        pass

# Implementar y probar el sistema completo
def main_integration_exercise():
    """
    Ejercicio principal que integra todos los conceptos
    """
    # TODO: Instanciar sistema de recomendaciones
    # TODO: Cargar datos sint√©ticos o reales
    # TODO: Entrenar modelos
    # TODO: Evaluar rendimiento
    # TODO: Optimizar hyperpar√°metros
    # TODO: Generar recomendaciones de prueba
    # TODO: Crear visualizaciones de resultados
    pass

if __name__ == "__main__":
    main_integration_exercise()
```

## Recursos adicionales y pr√≥ximos pasos

### Librer√≠as avanzadas para explorar

```python
# TODO: Explorar estas librer√≠as despu√©s del laboratorio

# Para computaci√≥n num√©rica avanzada
# import numba  # JIT compilation
# import cupy   # GPU arrays
# import dask   # Parallel computing

# Para ML avanzado
# import xgboost
# import lightgbm
# import optuna  # Hyperparameter optimization

# Para deep learning
# import torch
# import tensorflow

# Para visualizaci√≥n avanzada
# import plotly
# import bokeh
# import altair

# Para deployment
# import fastapi
# import streamlit
# import mlflow
```

## Criterios de evaluaci√≥n

Su progreso ser√° evaluado en base a:

### Dominio de Python (25%)
- ‚úÖ Uso correcto de estructuras de datos
- ‚úÖ Programaci√≥n funcional y OOP
- ‚úÖ Manejo de errores y debugging
- ‚úÖ Optimizaci√≥n de c√≥digo

### Manipulaci√≥n de datos (25%)
- ‚úÖ Competencia en NumPy y Pandas
- ‚úÖ Limpieza y transformaci√≥n de datos
- ‚úÖ Manejo eficiente de memoria
- ‚úÖ Procesamiento de datos grandes

### Machine Learning (25%)
- ‚úÖ Implementaci√≥n de algoritmos desde cero
- ‚úÖ Comprensi√≥n de conceptos fundamentales
- ‚úÖ Evaluaci√≥n y validaci√≥n de modelos
- ‚úÖ Pipeline completo de ML

### Mejores pr√°cticas (25%)
- ‚úÖ C√≥digo limpio y documentado
- ‚úÖ Testing y validaci√≥n
- ‚úÖ Optimizaci√≥n de rendimiento
- ‚úÖ Consideraciones de producci√≥n

## Conclusi√≥n

Este laboratorio intensivo le ha proporcionado una base s√≥lida en Python para Machine Learning. Los conceptos y t√©cnicas practicados aqu√≠ son fundamentales para cualquier proyecto de ML en producci√≥n.

### Pr√≥ximos pasos recomendados:

1. **Profundizar en librer√≠as especializadas** (XGBoost, PyTorch, etc.)
2. **Practicar con datasets reales** de su dominio de inter√©s
3. **Implementar proyectos end-to-end** con deployment
4. **Contribuir a proyectos open source** de ML
5. **Mantenerse actualizado** con las √∫ltimas tendencias

---

**¬°Felicitaciones por completar esta sesi√≥n intensiva!** üêçüöÄ

Ahora tiene las herramientas y conocimientos necesarios para abordar proyectos de Machine Learning complejos con confianza.
